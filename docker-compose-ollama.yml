services:
  redis:
    image: "redis:alpine"
    container_name: redis
    ports:
      - "6379:6379"

  llm_proxy:
    build: ./llm_proxy_image
    container_name: llm_proxy
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_HOST=redis
      - LOCAL_MODEL_URL=http://ollama:11434/api/generate
      - PYTHONUNBUFFERED=1
    depends_on:
      redis:
        condition: service_started
      ollama:
        condition: service_healthy

  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "ollama list | grep -q 'llama3' && echo 'Healthy' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 18
      start_period: 1m
    entrypoint: ["/bin/sh", "-c", "
      ollama serve & \
      sleep 5 && ollama pull llama3 & tail -f /dev/null"
    ]

  litellm:
    environment:
      - OLLAMA_API_BASE=http://ollama:11434
    volumes:
      - ./litellm_proxy_server/ollama-config.yaml:/app/config.yaml
    depends_on:
      ollama:
        condition: service_healthy