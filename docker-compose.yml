services:
  redis:
    image: "redis:alpine"
    container_name: redis
    ports:
      - "6379:6379"

  llm_proxy:
    build: ./llm_proxy_image
    container_name: llm_proxy
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_HOST=redis
      - LOCAL_MODEL_URL=http://local_llm:11434/api/generate
      - PYTHONUNBUFFERED=1
    depends_on:
      - redis

  local-llm:
    image: ollama/ollama
    container_name: local_llm
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: always
    entrypoint: ["/bin/sh", "-c", "ollama serve && sleep 5 && ollama pull llama3"]